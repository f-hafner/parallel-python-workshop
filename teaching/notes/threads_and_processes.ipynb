{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threads and processes\n",
    "\n",
    "An alternative option for parallelization is the built-in `threading` module in python. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating from before\n",
    "import random \n",
    "\n",
    "def calc_pi(N, name=None):\n",
    "    \"Compute pi using N random samples\"\n",
    "    printing = name is not None\n",
    "    if printing:\n",
    "        print(f\"{name}: starting\")  \n",
    "    M = 0 \n",
    "    for i in range(N):\n",
    "        # Simulate random coordinates\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "        if x**2 + y**2 < 1: # don't need sqrt b/c 1**2 = 1\n",
    "            M += 1\n",
    "\n",
    "    if printing:\n",
    "        print(f\"{name}: Done\")\n",
    "    return 4*M/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.68 s, sys: 3.48 ms, total: 5.68 s\n",
      "Wall time: 5.68 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.1408208"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "calc_pi(10**7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 1: starting\n",
      "Thread 2: starting\n",
      "Thread 2: Done\n",
      "Thread 1: Done\n",
      "CPU times: user 5.86 s, sys: 7.84 ms, total: 5.86 s\n",
      "Wall time: 5.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "n=int((10**7)/2)\n",
    "t1 = Thread(target=calc_pi, args=(n, \"Thread 1\", ))\n",
    "t2 = Thread(target=calc_pi, args=(n, \"Thread 2\", ))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "a1 = t1.join()\n",
    "a2 = t2.join()\n",
    "\n",
    "# TODO: collect the result and show number of computations and compute pi? -- see examples later on?\n",
    "# https://stackoverflow.com/questions/6893968/how-to-get-the-return-value-from-a-thread\n",
    "# Is it correct that the point here is that this is run sequentially and there is no speedup?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: where is the speedup\n",
    "\n",
    "- ask in the group?\n",
    "\n",
    "**Solution**\n",
    "- Python only allows one thread to acces the interpreter at any given time. In other words, if we have a python session and start two threads, only one of them can execute python code at the time.\n",
    "- This means that the two threads are waiting for the other to finish their work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from realpython\n",
    "\n",
    "- In python, a daemon thread shuts down immediately when a program exits. We can achieve this behavior when using `daemon=True` on the `threading.Thread` function. \n",
    "- If it is not specified, a thread is running in the background. It is only finished when `join` is called on it. Either this is done by us, or it is done when the program exits because python will close all objects, and on `threading` objects, the `_shutdown` method calls join under the hood.\n",
    "- If we call `join`, the main thread waits for the thread to finish. Whether `daemon=True` does not matter because the process is waited for to finish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the Global Interpreter Lock\n",
    "- The above did not run in parallel. The reaons: python's global interpreter lock\n",
    "- It prevents us from using multiple cores from a single python instance.\n",
    "- This makes programming in python safer (intuition???), but leads us to waste precious CPU resources.\n",
    "- We can circumvent or lift the GIL with two types of solutions \n",
    "    1. run multiple python instances: `multiprocessing`\n",
    "    2. have important code outside python: C++ extensions, cython, numba \n",
    "\n",
    "\n",
    "**Having multiple python instances**\n",
    "The problem is that we need to replicate program state between processes -- that is, if we load up a big dataset into memory and then run parallel processes on it, we need to transfer this big dataset to each child process. This is done with serialization (pickle, json) and creates large overhead. This is why multiprocessing should not be the first choice for parallelization.\n",
    "\n",
    "**Code outside python**\n",
    "Numpy has many routines that are situated outside of the GIL. Also numba makes this very easy, as we will show now.\n",
    "\n",
    "\n",
    "<mark>**Lesson learned: try out and profile your application!**</mark>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True, nogil=True)\n",
    "def calc_pi_nogil(N):\n",
    "    M = 0 \n",
    "    for i in range(N):\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "        if x**2 + y**2 < 1:\n",
    "            M += 1 \n",
    "    return 4 * M / N \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.07 ms ± 109 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "time_nogil = %timeit -o calc_pi_nogil(10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577 ms ± 11 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "time_forloop = %timeit -o calc_pi(10**6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "In general, it is good practice to use `@numba.jit` with `noypython=True`, to make sure the code is run without running any python objects. There is also a more direct way to do this: `numba.njit`.\n",
    "If we use `nopython=True` and numba is not able to run the required code without any python, it will raise an error. If we do not specify `nopython=True`, it may fall back to object code (which is in python?), causing slowdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using nogil is 81.63 times faster than using the for loop\n"
     ]
    }
   ],
   "source": [
    "speedup = time_forloop.average / time_nogil.average\n",
    "print(f\"Using nogil is {speedup:.2f} times faster than using the for loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=False)\n",
    "def calc_pi_nogil_wrong(N):\n",
    "    M = 0 \n",
    "    for i in range(N):\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "        if x**2 + y**2 < 1:\n",
    "            M += 1 \n",
    "    return 4 * M / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1264"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pi_nogil_wrong(10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.73 ms ± 178 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "time_wrong = %timeit -o calc_pi_nogil_wrong(10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `time_wrong` and `time_nogil` take about the same time. Why is that?\n",
    "\n",
    "Note on numba options \n",
    "- `nogil` does not have any effect if we run it just like that above -- it's only when we start threading that we can see the speedup.\n",
    "    - This will be shown in the asyncio section; would it make sense to also show it here? \n",
    "- `nopython=False` **still tries to compile to machine code**, but *does not require it*; but because the simple example above succeeds in compiling to use no python objects at all, we do not see a difference in speed when using `nopython=False` and `nopython=True`.\n",
    "\n",
    "-> the exercise below is kind of in this spirit: numpy always (?) releases the GIL. So I could make this more clear then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Threading on a numpy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "a = np.random.random(10**6)\n",
    "b = np.random.random(10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 ms ± 3.55 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10 -r 10\n",
    "np.sort(a)\n",
    "np.sort(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.4 ms ± 399 µs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10 -r 10\n",
    "t1 = Thread(target=np.sort, args=(a, ))\n",
    "t2 = Thread(target=np.sort, args=(a, ))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "a1 = t1.join()\n",
    "a2 = t2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a speed-up of about 50%, which we get because we process the threads in parallel in numpy releases the GIL. \n",
    "The same we could achieve with numba (or with numpy) for the `calc_pi` function. Should we do this as an additional exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "\n",
    "We can run multiple processes in parallel with the `multiprocessing` module. Its API is similar to the one from threading, but its behavior is quite different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 1: starting\n",
      "Process 2: starting\n",
      "Process 1: Done\n",
      "Process 2: Done\n",
      "CPU times: user 11.6 ms, sys: 87 µs, total: 11.7 ms\n",
      "Wall time: 3.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # n = 10**6\n",
    "    n=int((10**7)/2)\n",
    "    p1 = Process(target=calc_pi, args=(n, \"Process 1\"))\n",
    "    p2 = Process(target=calc_pi, args=(n, \"Process 2\"))\n",
    "\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n",
    "    p1.join()\n",
    "    p2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is going on?\n",
    "\n",
    "- In contrast to `threading`, we managed to get a speed up here of a bit less than 50 percent\n",
    "- **But**, under the hood, two new processes with a fresh copy of the python interpreter are created; and all resources associated to the parent are transferred\n",
    "- Creating a process is resources intensive, multiprocessing is only beneficial if running the function is larger than the overhead of creating a new process\n",
    "    - In the present context, this seems to be true since there are few objects to be transferred between processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipped for now: passing objects & sharing state; using contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "!python teaching/notes/mp_queue.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting tasks to a pool: role of overhead\n",
    "\n",
    "Describe task; write to exercise document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "Make two python files\n",
    "\n",
    "To vary the amount of work: `mp_pool.py`\n",
    "\n",
    "```python\n",
    "\"Vary the amount of work\"\n",
    "\n",
    "\n",
    "from itertools import repeat\n",
    "import multiprocessing as mp\n",
    "from timeit import timeit\n",
    "from calc_pi import calc_pi\n",
    "\n",
    "\n",
    "def submit(ctx, N):\n",
    "    with ctx.Pool() as pool:\n",
    "        pool.starmap(calc_pi, repeat((N,), 4))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    for i in (100, 1_000, 10_000, 1_000_000, 10_000_000): # note true N is 4*this input, but same order of magnitude\n",
    "        res = timeit(lambda: submit(ctx, i), number=5)\n",
    "        print(f\"Using {i} samples took {res} seconds.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To vary the amount of workers: `mp_pool_vary_processes`\n",
    "\n",
    "```python\n",
    "\"Vary the amount of workers\"\n",
    "from itertools import repeat\n",
    "import multiprocessing as mp\n",
    "from timeit import timeit\n",
    "\n",
    "from calc_pi import calc_pi\n",
    "\n",
    "\n",
    "def submit(ctx, n_procs):\n",
    "    with ctx.Pool() as pool:\n",
    "        pool.starmap(calc_pi, repeat((1_000_000//n_procs,), n_procs))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    for i in (1, 2, 4, 8, 16): \n",
    "        res = timeit(lambda: submit(ctx, i), number=5)\n",
    "        print(f\"Using {i} workers took {res} seconds.\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 100 samples took 0.3531035240011988 seconds.\n",
      "Using 1000 samples took 0.29756459900090704 seconds.\n",
      "Using 10000 samples took 0.33205998300400097 seconds.\n",
      "Using 1000000 samples took 3.531531136999547 seconds.\n",
      "Using 10000000 samples took 39.767159585004265 seconds.\n"
     ]
    }
   ],
   "source": [
    "!python teaching/notes/mp_pool.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 workers took 3.1046130419999827 seconds.\n",
      "Using 2 workers took 1.7845441090030363 seconds.\n",
      "Using 4 workers took 1.1417671239978517 seconds.\n",
      "Using 8 workers took 1.2529043550021015 seconds.\n",
      "Using 16 workers took 1.3137299840018386 seconds.\n"
     ]
    }
   ],
   "source": [
    "!python teaching/notes/mp_pool_vary_processes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lessons learned \n",
    "\n",
    "**Varying the amount of work**\n",
    "- overhead from creating processes. fixed cost -> for low `N`, the overhead dominates, and we need high enough `N` to make this worthwhile.\n",
    "- with larger samples (`1_000_000` -> `10_000_000`), a 10x increase in the amount of work results in a more than 10x increase in the time taken.\n",
    "\n",
    "\n",
    "**Varying the number of processes**\n",
    "- If our task is large enough (`N = 1_000_000`), it makes sense to spread across multiple workers. We get a speed-up. \n",
    "    - Compare this to `N = 100`, where there is barely any speed-up as we increase the number of workers. This is because of the overhead from creating the python processes.\n",
    "- There are limits because of CPU congestion (only have 4 physical cores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 77 ms, sys: 0 ns, total: 77 ms\n",
      "Wall time: 93.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.13896"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "calc_pi(100_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
